\def\year{2020}\relax

\documentclass[letterpaper]{article}

\usepackage{aaai20}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}

\usepackage{graphicx}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\usepackage{amsmath,amssymb,amsthm}
\usepackage[vlined,algoruled,titlenumbered,noend,portugues]{algorithm2e}
\usepackage{booktabs}

\graphicspath{{./images/}}

\pdfinfo{
  /Title (Relatório 02 - Algoritmos de Aprendizado por Reforço)
  /Author (Daniel Baptista Dias)
}

% /Title (Relatório 01 - Algoritmos de Planejamento Probabilístico)
% /Author (Daniel Baptista Dias)

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai20.sty is the style file for AAAI Press proceedings, working notes, and technical reports.
\setlength\titlebox{2.5in} % If your paper contains an overfull \vbox too high warning at the beginning of the document, use this
% command to correct it. You may not alter the value below 2.5 in

\title{Relatório 02 - Algoritmos de Aprendizado por Reforço}
\author{Daniel Baptista Dias}

\begin{document}

\maketitle

\section{Introdução}
\label{sec:introducao}

Na disciplina de Inteligência Artificial existem algumas áreas que estudam formas de como um agente pode tomar um conjunto 
de decisões em sequência interagindo com um ambiente com o objetivo de solucionar um problema da melhor forma possível.

Uma forma de modelar esta situação é utilizando os Processos Markovianos de Decisão (MDP, do inglês \textit{Markov Decision Processes})\cite{Puterman-1994}, onde é assumido o resultado da interação do agente com o ambiente é incerto, e que uma ação realizada pelo agente neste ambiente em uma situação (um estado do ambiente) pode ter diferentes resultados, cuja frequência é ditada por uma distribuição de probabilidades, e retorna uma recompensa para o agente.

No trabalho anterior o objeto de estudo foi a área de Planjemamento Probabilístico que busca dado o conhecimento de um MDP, busca encontrar qual é a melhor forma de agir dado cada situação possível neste ambiente. Porém em algumas situações encontrar essa melhor solução pode ser complexo, seja por o número de situações possíveis ser muito alto, seja por não termos muitos dados do ambiente para poder montar uma estratégia nele.

A área de Aprendizado por Reforço busca atacar esses problemas assumindo que o agente poderá interagir nesse ambiente mesmo sem um conhecimento (ilustrada pela figura \ref{fig:rf-agent-interaction}), de forma que ele possa executar ações nele e a medida que vá interagindo com o ambiente, vá aprendendo mais características (estados, transições e recompensas) sobre ele e vá buscando encontrar melhores maneiras de agir.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{rf-agent-interaction}
  \caption{Interação de um agente com o seu ambiente}
  \label{fig:rf-agent-interaction}
\end{figure}

Neste trabalho, para efeito de estudo, serão feitos alguns experimentos com os algoritmos baseados em valor, em busca de política e com reuso de experiências.

\section{Arcabouço Teórico}

No contexto de tomada de decisões por um agente em um ambiente completamente observável, um Processo de Decisão Markoviano (MDP) pode ser descrito por uma tupla $\mathcal{M}=\langle S,A,R,P,\gamma \rangle$, onde:

\begin{itemize}
    \item $S$ é um conjunto de estados finitos e discretos, que definem a situação do ambiente em um determinado momento;
    \item $A$ é um conjunto de ações que o agente pode executar;
    \item $R : S \rightarrow \mathcal{R} $ é a função recompensa, que retorna a recompensa obtida pelo agente ao se alcançar um determinado estado;
    \item $P : S \times A \times S \rightarrow [0, 1]$ é uma função de transição que retorna a probabilidade de um agente, dado que executou uma ação $a \in A$ em um estado $s \in S$, alcançar o estado $s' \in S$. Neste trabalho uma notação que também será usada para representar essa transição é $P(s'|s,a)$.
\end{itemize}

Neste ambiente a tomada de decisão ocorre por etapas (estágios), onde o agente executa uma ação de cada vez alterando o estado do ambiente e recebendo uma recompensa ou sendo penalizado com um custo (recompensa negativa). Estes estágios são denominados pela notação $t \in \{ 1, \dots, T \} $, onde caso $T$ seja um número finito, consideramos o problema como um problema de \textbf{horizonte finito}.

O objetivo do agente neste MDP é encontrar uma política $\pi : S \rightarrow A$ que indique a melhor ação a ser tomada em cada estado $s$ a fim de se obter a maior recompensa possível.

Para identificar esta política pode-se calcular o valor dela para cada estado $s$ através do critério da recompensa esperada total. Esse critério indica o quanto de recompensa um agente pode receber em média a executar uma política $\pi$ a partir do estado $s_0 = s$ até o instante $T$ dado um fator de desconto $\gamma$ (limitado ao intervalo $[0, 1]$), calculada pela função valor $V : S \rightarrow \mathcal{R} $ para a política $\pi$:

\begin{equation} \label{eq:total_expected_reward}
    V_\pi(s) = E_{\pi} \left[ \sum_{t=0}^{T} \gamma^t r_t | s_0 = s \right]
\end{equation}

Uma política ótima $\pi^*$ para um MDP é aquela que tem o maior valor entre todas as outras políticas possíveis para cada estado, ou seja: $V_{\pi^*}(s) \geq V_{pi'}(s), \forall s,\pi'$.

O valor da política ótima $V_{\pi^*}$ pode ser encontrado a partir função valor ótima $V^* = V_{\pi^*}$ definida pela equação de Bellman \cite{Bellman-1966}, que encontra uma função valor que maximize as recompensas esperadas para todo $s \in S$:

\begin{equation} \label{eq:bellman_equation}
    V^*(s) = R(s) + \max_{a \in A} \left\{ \gamma \sum_{s'\in S} P(s'|s,a)V^*(s') \right\}
\end{equation}

Existem algoritmos que trabalham com essa abordagem para se obter a política ótima, usualmente resolvendo a equação de Bellman através de algoritmos de \textit{programação dinâmica} como o \textit{Iteração de Valor} e a \textit{Iteração de Política} \cite{Howard-1960} de forma eficiente. Neles, aplicamos um operador de Bellman para cada estado, descrito pela equação \ref{eq:bellman_operator} abaixo: 

\begin{equation} \label{eq:bellman_operator}
  V^{t+1}(s) = (BV^t)(s) = \max_{a \in A} \left\{ Q^{t+1}(s,a) \right\}
\end{equation}

onde $ Q^{t+1}(s,a) $ é considerado uma função qualidade, definida por:

\begin{equation} \label{eq:quality_function}
  Q^{t+1}(s,a) = R(s) + \gamma \sum_{s'\in S} P(s'|s,a)V^t(s')
\end{equation}

Entretanto para alguns tipo de problema eles ter uma aplicabilidade limitada devido ao problema da \textit{maldição da dimensionalidade}: o fato que o número de estados pode ser proibitivamente grande dependendo da modelagem dos estados (por exemplo, caso um estado precise modelar muitas variáveis com diferentes combinações de valores) \cite{SuttonBarto-2018}. 

Uma forma de tentar evitar o problema do crescimento do espaço de estados é ao invés de usar um conhecimento completo do ambiente para resolver-lo, buscar uma política ótima através do uso da experiência: a simulação ou interação de um agente agindo no ambiente em sequência, amostrando uma sequência de estados, ações e recompensas. Os métodos de \textit{Monte Carlo} buscam justamente fazer isso, baseando a tomada de decisão em cima das amostragens média dessa experiência ao invés de utilizar a equação de Bellman, obtendo uma estimativa da função valor $\hat{V}(s)$ e da função qualidade $\hat{Q}(s,a)$, concentrando o uso os recursos computacionais nos estados e ações mais relevantes para a resolução do problema.

%TODO: falar sobre o dilema exploração / exploitação

Esse tipo de algoritmos garante uma solução $\epsilon$-ótima independente do tamanho do espaço de estados ($|S|$) mostrando que a probabilidade de falha de convergência pode ser limitada segundo a desigualdade de Hoeffding \cite{Hoeffding-1994}. Nesses algoritmos é mostrando que caso haja uma quantidade suficientemente grande de episódios (amostras de um agente executando uma política $\pi_{\hat{Q}}$ em um ambiente) essa probabilidade de falha de convergência tende a zero. Um algoritmo que baseia nessa idéia de convergência é o UCT \cite{KocsisSzepesv-2006}.

Combinando as idéias dos métodos de Monte Carlo e de programação dinâmica (baseados no cálculo do operador de Bellman) o método de \textit{diferença temporal} buscam calcular a função valor $V$ com base na experiência de um agente em um ambiente, que através da aproximação estocástica \cite{RobbinsMonro-1951}, estima a função valor para um instante $t$, estado $s_t$, recompensa recebida $r_t$ e  uma taxa de aproximação a $V(s_t)$ (também chamada de taxa de aprendizado) $\alpha \in (0, 1])$ como:

\begin{equation} \label{eq:temporal_difference}
  V(s_t) = V(s_t) + \alpha [ r_t + \gamma V(s_{t+1}) - V(s_t) ]
\end{equation}

Dado uma política $\pi$ qualquer e um horizonte $T$, pode-se aplicar este método para avaliar esta política e encontrar $V_{\pi}$, em um algoritmo com garantia de convergência chamado \textsc{TD(0)}. Ele será a base para dois algoritmos que serão apresentados posteriormente, o \textit{SARSA} e o \textit{Q-Learning} \cite{Watkins-Dayan-1992}. Por necessitar armazenar os valores de $V(s)$ para cada estado, esses algoritmos tamb;em são chamados de métodos tabulares.

Mesmo não precisando iterar por todo o espaço de estados, armazenar e calcular $V(s)$ e $Q(s,a)$ pode demandar de muitos recursos computacionais (por exemplo, ao tentar armazenar um estado contínuo sem nenhuma técnica de discretização). Uma forma de resolver este problema é ao invés de trabalhar com uma aproximação da função valor $ \hat{V}(s, \theta) \approx V_{\pi}(s)$, onde ao invés de se armazenar os valores de $s$ serão armazenados o vetor de pesos $\theta \in \mathcal{R}^d$, onde $d$ seja menor que $|S|$. Nesta abordagem, os algoritmos se focarão em aprender os valores de $\theta$ de forma a encontrarem uma política ótima para a interação com o ambiente. Os algorimos de busca no espaço de políticas, \textit{REINFORCE com baseline}, e o que trabalha com reuso de experiências, o \textit{Fitt-Q}, partem desses principios para o aprendizado.

\section{Algoritmos}

Neste trabalho foram implementados dois algoritmos \textit{value-based} que utilizam o método de diferença temporal para identificar uma política ótima em um ambiente: os algoritmos \textit{SARSA} e \textit{Q-Learning} \cite{Watkins-Dayan-1992}, um algoritmo que trabalha com busca no espaço de políticas, o \textit{REINFORCE com baseline} e um que trabalha com reuso de experiências (\textit{sample efficient}), o \textit{Fitt-Q} \cite{Riedmiller-2005}.

\subsection{SARSA}

Partindo das técnicas de diferença temporal, a prova de convergência de diferença temporal para $V(s_t)$ no algoritmo \textsc{TD(0)} também se aplica para o aprendizado de $Q(s, a)$, onde dado um estado não terminal $s$ e simulando uma ação $a$ de política $\pi$, podemos atualizar $Q(s,a)$ segundo a equação \ref{eq:time_diff_update_sarsa}:

\begin{multline} \label{eq:time_diff_update_sarsa}
  \textsc{TimeDiff}(Q, s, a, s', \gamma, \alpha) = \\ 
  Q(s,a) = Q(s,a) + \alpha [r + \gamma Q(s', a') - Q(s,a)]
\end{multline}

Nesse algoritmo durante $T$ episódios o \textsc{SARSA} (Algoritmo \ref{alg:sarsa}) irá amostrar pares de estado-ação para cada instante $t \in \{ 1, \dots, T \}$ e os atualizará com a diferença temporal. Caso seja encontrado um estado terminal, o valor de $Q(s,a)$ será atualizado para zero e o episódio será encerrado. Como esta regra considera a quintupla $ (s, a, r, s', a') $, ela dá o nome ao próprio algoritmo \textsc{SARSA} (Algoritmo \ref{alg:sarsa}). Nele o método \textsc{TimeDiff} justamente aplica a Equação \ref{eq:time_diff_update_sarsa}. 

A tomada de decisão de uma ação nesse algorito se dá seguindo uma política $\epsilon$-gulosa sobre o valor de $Q$, onde o algoritmo \textit{explora} o espaço de estados selecionando uma ação de maneira uniforme com probabilidade $\epsilon$ e explota (escolhe a ação com base no critério $a = \arg \max_{a' \in A} Q(s, a') $) as ações com probabilidade $1 - \epsilon$. Esse método de escolha é descrito no algoritmo pelo método \textsc{ChooseActionFromPolicy}.

Durante a fase de exploração, ao escolher a ação baseada na política para atualizar o valor o estado (segmento $r + \gamma Q(s', a') - Q(s,a)$ da equação \ref{eq:time_diff_update_sarsa}), consideramos o \textsc{SARSA} um algoritmo de controle \textit{on-policy}, ou seja, ele explora em cada episódio e a atualiza os valores dos estados sempre considerando a política $\epsilon$-gulosa atual.

As propriedades de convergência deste algoritmo dependem justamente dessa natureza dele onde o valor de $Q$ depende da política seguida. Ele converge com probabilidade 1 seguindo uma política $\epsilon$-gulosa garantindo que todos os pares de estado-ação sejam visitados infinitas vezes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linesnumbered
\dontprintsemicolon
\begin{algorithm}[t!]
{
	\caption{\textsc{SARSA}($ env, T, \gamma, \alpha, \epsilon $)}
	\label{alg:sarsa}
    $Q(s,a) := 0, \forall s \in S, a \in A $\\

    \ParaCada{$ t \in \{ 1, \dots, T \} $}
    {
      $s := env.start()$\\
      $a := \textsc{ChooseActionFromPolicy}(Q, \epsilon, s) $\\

      \Enqto{$ \neg env.done() $}
      {
        $s', r := env.step(a)$\\
        $a' := \textsc{ChooseActionFromPolicy}(Q, \epsilon, s') $\\
        $Q(s, a) = \textsc{TimeDiff}(Q, s, a, s', a', \gamma, \alpha)$\\
        $s := s'$ \\
        $a := a'$ \\
      }
    }

    \Retorna{$Q$}
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Q-Learning}

O algoritmo \textsc{Q-Learning} (Algoritmo \ref{alg:q-learning}) segue a mesma idéia de atualização da função $Q(s,a)$ através da diferença temporal, porém utilizando uma estratégia \textsc{off-policy}. Ao invés de atualizar o estado na fase de exploração considerando uma ação da política $\epsilon$-gulosa atual, ele considera o valor de $Q(s,a)$ da melhor ação naquele momento, considerando a atualização de diferença temporal como a equação \ref{eq:time_diff_update_q_learning}:

\begin{multline} \label{eq:time_diff_update_q_learning}
  \textsc{TimeDiff}(Q, s, a, s', \gamma, \alpha) = \\ 
      Q(s,a) = Q(s,a) + \alpha [r + \gamma max_a' Q(s', a') - Q(s,a)]
\end{multline}

Note que nesta versão do update, o método \textsc{TimeDiff} não necessita mais de $a'$, por considerar agora o maior valor $Q$ dentre as ações disponíveis. Essa alteração simplifica as provas de convergência e a sua análise e vai servir de base para outros algoritmos baseados em Q-Learning, como o \textsc{Fitt-Q} que será mostrado adiante.

Exceto pela atualização por diferenção temporal, o restante do algoritmo é semelhante ao \textsc{SARSA}: durante a fase de exploração a escolha da próxima ação segue a política $\epsilon$-gulosa atual e as propriedades de convergência se mantêm as mesmas, é necessário visitar todos os pares de estado-ação infinitas vezes para convergir por probabilidade 1.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linesnumbered
\dontprintsemicolon
\begin{algorithm}[t!]
{
	\caption{\textsc{Q-Learning}($ env, T, \gamma, \alpha, \epsilon $)}
	\label{alg:q-learning}
    $Q(s,a) := 0, \forall s \in S, a \in A $\\

    \ParaCada{$ t \in \{ 1, \dots, T \} $}
    {
      $s := env.start()$\\

      \Enqto{$ \neg env.done() $}
      {
        $a := \textsc{ChooseActionFromPolicy}(Q, \epsilon, s) $\\
        $s', r := env.step(a)$\\
        $Q(s, a) = \textsc{TimeDiff}(Q, s, a, s', \gamma, \alpha)$\\
        $s := s'$ \\
      }
    }

    \Retorna{$Q$}
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{REINFORCE}

O algoritmo \textsc{REINFORCE} trabalha a partir das abordagens de aproximação de função, neste caso aprendendo um vetor de pesos $\theta \in \mathcal{R}^d$, com um $d < |S \times A|$ que aproxime uma função de política $P(s|a, \theta)$, ou seja realizando uma busca no espaço de políticas. Nele é considerada uma função avaliadora $J(]theta)$ de forma a se se possa aproximar com um método de gradiente ascendente en $J$:

\begin{equation}
  \theta_{t+1} = \theta_t + \alpha \widehat{\nabla J(\theta_t)}
\end{equation}

Onde $\widehat{\nabla J(\theta_t)} \in \mathcal{R}^d$ é uma estimativa estocástica cuja esperança aproxima o gradiente da medida de performance com respeito a $\theta_t$ \cite{SuttonBarto-2018}.

Pelo teorema do gradiente de política pode-se mostrar que $\nabla J(\theta_t)$ e o update de $\theta$ podem ser escritos como:

\begin{equation}
  \label{eq:reinforce_theta_j_gradient}
  \nabla J(\theta_t) = E_{\pi} \left[ G_t \frac{\nabla_{\pi}(a_t|s_t, \theta)}{\pi(a_t|s_t, \theta)} \right]
\end{equation}

\begin{equation}
  \label{eq:reinforce_theta_approximation}
  \theta_{t+1} = \theta_t + \alpha G_t \frac{\nabla_{\pi}(a_t|s_t, \theta_t)}{\pi(a_t|s_t, \theta_t)} 
\end{equation}

Onde $G_t$ é o retorno das recompensas descontados que um agente recebe ao seguir uma política $\pi(\cdot | \cdot, \theta)$ ao longo do tempo. Por fim, para a regra de atualização de $\theta_{t+1}$ pode-se substituir $\frac{\nabla_{\pi}(a_t|s_t, \theta)}{\pi(a_t|s_t, \theta)}$ por $ \nabla \ln \pi(a_t, s_t, \theta_t) $.

Com a equação de aproximação em mãos, o \textsc{REINFORCE} pode ser visto no Algoritmo \ref{alg:reinforce}, onde para cada episódio se amostra uma sequência triplas de estado, ação e recompensa para uma política $\pi(\cdot | \cdot, \theta)$ e essa sequência é utilizada para atualizar $\theta$ segundo a equação \ref{eq:reinforce_theta_approximation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linesnumbered
\dontprintsemicolon
\begin{algorithm}[t!]
{
	\caption{\textsc{Reinforce}($ env, T, \gamma, \alpha $)}
	\label{alg:reinforce}
    inicializa $ \theta $ com valores arbitrários \\

    \ParaCada{$ episode \in \{ 1, \dots, T \} $}
    {
      $episodeSteps := \textsc{GenerateEpisode}(env, \theta)$\\

      \ParaCada{$s_t, r_t, a_t, t \in episodeSteps$}
      {
        $G := \sum_{k = t+1}^T \{ \gamma^{k-t-1} episodeSteps[k].r \}$\\
        $ \theta := \theta + \alpha \gamma^t G \nabla \ln \pi(a_t|s_t, \theta) $
      }
    }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Além da aproximação da política utilizando um $\theta$, uma outra versão do \textsc{REINFORCE}, vista no algoritmo \ref{alg:reinforce_baseline}, pode também utilizar outra função de aproximação, um baseline $b(s_t)$, que pode acelerar o aprendizado de $\theta$ (equação \ref{eq:reinforce_theta_approximation_baseline}). No caso do algoritmo \textsc{REINFORCE-B}, este baseline é uma estimativa da função valor $b(s_t) = \hat{v}(s, w)$, onde o parâmetro $w$ também é aprendido ao longo da execução do algoritmo.

Neste trabalho o código do \textit{REINFORCE com baseline} utiliza redes neurais para implementar o aprendizado de $\theta$ e $w$.

\begin{equation}
  \label{eq:reinforce_theta_approximation_baseline}
  \theta_{t+1} = \theta_t + \alpha (G_t - b(s_t)) \frac{\nabla_{\pi}(a_t|s_t, \theta_t)}{\pi(a_t|s_t, \theta_t)} 
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linesnumbered
\dontprintsemicolon
\begin{algorithm}[t!]
{
	\caption{\textsc{Reinforce-B}($ env, T, \gamma, \alpha_{\theta}, \alpha_{w} $)}
	\label{alg:reinforce_baseline}
    inicializa $ \theta $ e $ w $ com valores arbitrários \\

    \ParaCada{$ episode \in \{ 1, \dots, T \} $}
    {
      $episodeSteps := \textsc{GenerateEpisode}(env, \theta)$\\

      \ParaCada{$s_t, r_t, a_t, t \in episodeSteps$}
      {
        $G := \sum_{k = t+1}^T \{ \gamma^{k-t-1} episodeSteps[k].r \}$\\
        $ \delta := G - \hat{v}(s_t, w) $\\
        $ w := w + \alpha_w \delta \nabla \hat{v}(s_t, w) $\\
        $ \theta := \theta + \alpha_{\theta} \gamma^t \delta \nabla \ln \pi(a_t|s_t, \theta) $
      }
    }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Fitt-Q}

No algoritmo \textsc{Fitt-Q} \cite{Riedmiller-2005}, assim como nos algoritmos anteriores, a idéia é aprender os vetores de peso $\theta$ de forma que seja possível minimizar o erro quadrático médio de Bellman, conforme a equação \ref{eq:msbe}:

\begin{equation} \label{eq:msbe}
  \textsc{MSBE}(\theta, \theta') = || \hat{V}_{\theta} - \mathcal{T} \hat{V}_{\theta'} ||^2_D
\end{equation}

Para este otimização é preciso calcular a norma quadrática entre estimativas de $\hat{V}_{\theta}$ e $\hat{V}_{\theta'}$ aplicado ao operador de Bellman $\mathcal{T}$ considerando $D$ uma matriz diagonal contendo uma distribuição limite $d(s) = \lim_{t \rightarrow \infty} P(s_t = s)$, utilizando o algoritmo \ref{alg:fittq-theory}.

Ao invés de usar uma técnica de gradiente descendente que utilize uma taxa de aprendizado $\alpha$ para encontrar uma valor de $\theta$, o trabalho de \cite{Riedmiller-2005} propõe aplicar uma rede neural que reuse experiências de interação de um agente com o ambiente (episódios) para treinar esta rede e encontrar uma estimativa $\hat{Q}(s, a)$ que o agente possa utilizar para agir no ambiente. 

Neste algoritmo são considerados apenas os parâmetros $T_{train}$, o horizonte máximo de amostragem de um episódio para treinamento da rede neural, e $T_{eval}$, o horizonte máximo de amostragem de um episódio de avaliação da rede.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\linesnumbered
\dontprintsemicolon
\begin{algorithm}[t!]
{
	\caption{\textsc{Fitt-Q}($ env, T, \epsilon $)}
	\label{alg:fittq-theory}
    inicializa $ w_0 $ com valores arbitrários \\

    \ParaCada{$ episode \in \{ 1, \dots, T \} $}
    {
      $w_t := \arg \min_{w \in \mathcal{R}^k} \textsc{MSBE}(w, w_{t-1}) $ \\
      $res := || w_t - w_{t-1} ||$ \\

      \Se{$res < \epsilon$}
      {
        \textbf{interrompe loop}
      }
    }
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimentos e Resultados}

Os experimentos foram executados na plataforma Google Colab com o Python, na época deste relatório com 2 CPUs de 2.2 GHz e 13GB de memória. Parte dos algoritmos foi baseado em implementações existentes e estão disponíveis no Github\footnote{Disponível em https://github.com/danielbdias/automated-planning-and-reinforcement-learning-studies}. Os algoritmos base das implementações foram:

\begin{itemize}
  \item \textit{Q-Learning} e \textit{SARSA}: https://github.com/dennybritz/reinforcement-learning/tree/master/TD
  \item \textit{REINFORCE com baseline}: https://github.com/SwamyDev/reinforcement
  \item \textit{Fitt-Q}: https://github.com/seungjaeryanlee/implementations-nfq
\end{itemize}

As implementações que utilizaram redes neurais foram implementadas com as bibliotecas Tensorflow e PyTorch.

\subsection{Configuração dos experimentos}

O domínio utilizado foi o \textit{CartPole} \cite{SuttonBarto-2018}, que consiste em controlar um carrinho em movimento em um local delimitado que contém um espécie de mastro anexada a ele de pé que pode cair de acordo com o movimento do carrinho. Ele possui as seguintes características:

\begin{itemize}
  \item O estado observado é composto por quatro variáveis contínuas (exemplificadas na figura \ref{fig:cart-pole}): posição do carrinho, velocidade do carrinho, ângulo do mastro em relação ao carrinho e velocidade angular do mastro
  \item As ações possíveis são mover o carrinho para a esquerda ou para a direita, tentando equilibrar a alavanca de forma que ela não caia para um dos lados
  \item O agente recebe 1 de recompensa caso o mastro não tenha caído e o carrinho esteja em uma zona delimitada. Caso o mastro caia ou o carrinho saia da zona delimitada, a agente não recebe mais recompensa (recompensa 0)
\end{itemize}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{cart-pole}
  \caption{Exemplo do domínio \textit{CartPole}}
  \label{fig:cart-pole}
\end{figure}

Cada algoritmo foi executado 5 vezes para cada configuração de parâmetro, com as seguintes configurações: 3000 episódios, fator de desconto $\gamma = 0.9$ e os seguintes parâmetros específicos:

\begin{itemize}
  \item \textit{SARSA} e \textit{Q-Learning}: $\alpha \in \{ 0.1, 0.2, 0.5, 0.8 \}$ e $epsilon \in \{ 0.01, 0.05, 0.1, 0.2 \}$
  \item \textit{REINFORCE com baseline}: $\alpha_{\theta} \in \{ 5, 50 \}$ e $\alpha_{w} \in \{ 0.01, 0.1 \}$
  \item \textit{Fitt-Q}: $T_{train} \in \{ 100, 200, 300 \} $ e $T_{eval} = \{ 1000, 2000, 3000 \}$
\end{itemize}

Além destas parametrizações os algoritmos \textit{SARSA} e \textit{Q-Learning} utilizam uma técnica para a discretização de cada variável de estado onde o domínio de cada variável foi separado em sete pontos e cada variável poderia ter apenas o valor de um desses pontos, o ponto mais próximo do valor real dela. 

Para a avaliação de cada algoritmo foi medido a quantidade de passos com sucesso em relação aos episódios amostrados pelos algoritmos. Em todos os gráficos dos experimentos são mostradas as trajetórias médias das execuções de cada algoritmo. Como no domínio de testes o agente recebe um de recompensa para cada momento em que ele não deixa cair o mastro, a recompensa será igual a quantidade de passos tomados com sucesso e por isso não será considerada.

Os experimentos foram divididos em duas etapas: uma onde cada algoritmo foi executado 5 vezes para cada combinação de hiperparâmetros e suas trajetórias médias foram comparadas e outro onde cada um dos quatro algoritmos são comparados entre si. Para esta última comparação é levado em conta somente a execução que teve a melhor trajetória média  (i.e., a melhor média de passos com sucesso para os episódios).

\subsection{Resultados}

Ao todo foram realizadas 225 execuções de algoritmos em cerca de 4 horas, como pode ser visto na tabela \ref{table:hyperparameter-tuning-results}. Os algoritmos \textit{SARSA} e \textit{Q-Learning} tiveram tempo médio por execução parecidos com cerca de $3.2$ segundos, o \textit{Fitt-Q} foi ligeiramente mais devagar, com $4.5$ segundos de execução e o \textit{Reinforce} sendo cerca de 27 vezes mais devagar, executando em $108.92$ segundos.

\begin{table}[ht]
  \caption{Resultados das execuções para cada algoritmo}
  \label{table:hyperparameter-tuning-results}
  \begin{tabular}{lccc}
      \toprule
      Algoritmo  &  Execuções & Tempo médio      & Tempo total     \\
      {}         &  {}        & por execução (s) & de execução (s) \\
      \midrule
      SARSA      &         80 &           3.2175 &            1287 \\
      Q-Learning &         80 &           3.3275 &            1331 \\
      REINFORCE  &         20 &           108.92 &           10892 \\
      Fitt-Q     &         45 &             4.53 &            1020 \\
      \bottomrule
  \end{tabular}
\end{table}

Considerando as 5 combinações de hiperparâmetros que tiveram maior performance, pode-se ver nas Figuras \ref{fig:exp-sarsa-episodes} e \ref{fig:exp-qlearning-episodes} tanto o \textit{SARSA} quanto o \textit{Q-Learning} tiveram comportamentos parecidos, com resultados ruins nos primeiros episódios e ao longo do tempo aprendendo mais sobre o domínio e tendo melhores resultados, com mais passos com sucesso, até chegarem próximos de 100 passos com sucesso ao final dos 3000 episódios executados.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{exp-sarsa-episodes}
  \caption{Passos por episódio para o algoritmo SARSA}
  \label{fig:exp-sarsa-episodes}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{exp-qlearning-episodes}
  \caption{Passos por episódio para o algoritmo Q-Learning}
  \label{fig:exp-qlearning-episodes}
\end{figure}

Devido ao tempo extensivo de execução do \textit{REINFORCE com baseline} (chamado nesta seção somente como \textit{REINFORCE}), foram executados poucas combinações de hiperparâmetros. Entretanto os seus resultados se mostraram bons em relação aos algoritmos anteriores. Como pode ser visto na Figura \ref{fig:exp-reinforce-episodes}, duas combinações de parâmetros alcançaram episódios com 500 passos executados com sucesso e as outras duas com episódios com cerca de 300 passos. Nota-se também que o valor de $\alpha_{\theta}$ (taxa de aprendizado da política) influenciou bastante nesses resultados, com ambas as execuções com $\alpha_{\theta} = 50$ aprendendo boas políticas nos primeiros 1000 episódios e já atingindo episódios com 500 passos com sucesso.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{exp-reinforce-episodes}
  \caption{Passos por episódio para o algoritmo REINFORCE}
  \label{fig:exp-reinforce-episodes}
\end{figure}

O algoritmo \textit{Fitt-Q} já apresenta uma característica que nenhum dos outros algoritmos deste trabalho tem, por ter uma checagem de convergência, ele não precisa executar muitos episódios de execução para convergir. Em um dos casos o algoritmo utiliza em somente 175 iterações para convergir, alcançando um pouco mais de 250 passos com sucesso, utilizando $T_{train} = 100$ e $T_{eval} = 2000$. Um ponto interessante a ser observar nesses resultados é que entre os 5 melhores resultados, todos apresentam alguma combinação com $T_{train} = 100$, o que pode sugerir que experiências com até episódios podem ser o suficiente para o \textit{Fitt-Q} aprender boas soluções.

Entretanto a implementação deste algoritmo apresenta alguns pontos de melhoria: para rodar os experimentos ele utilizou uma versão modificada do domínio \textit{CartPole}, que considera custo ao invés de recompensa em seus cálculos e considera uma heurística de "dica" para alcançar os estados-meta e orientar o algoritmo (\textit{hint-to-goal}), o que pode ser uma das causas de número máximo de passos que este algoritmo simulou não ser tão bom quanto o \textit{REINFORCE}. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{exp-fittq-episodes}
  \caption{Passos por episódio para o algoritmo Fitt-Q}
  \label{fig:exp-fittq-episodes}
\end{figure}

Para a comparação entre os algoritmos foi utilizado somente e versão do algoritmo com os melhores hiperparâmetros, que pode ser vista na Tabela \ref{table:best-hyperparameters}. 

\begin{table}[ht]
  \caption{Melhores hiperparâmetros por algoritmo}
  \label{table:best-hyperparameters}
  \begin{tabular}{ll}
      \toprule
      Algoritmo  &                           Hiperparâmetros \\
      \midrule
      SARSA      &          $ \alpha = 0.2, \epsilon = 0.2 $ \\
      Q-Learning &          $ \alpha = 0.1, \epsilon = 0.2 $ \\
      Reinforce  & $ \alpha_{\theta} = 50, \alpha_w = 0.01 $ \\
      Fitt-Q     &      $ T_{train} = 100, T_{eval} = 2000 $ \\
      \bottomrule
  \end{tabular}
\end{table}

Por fim, pode-se ver na Figura \ref{fig:exp-allalgs-episodes}, os resultados das melhores execuções dos algoritmos. Percebe-se que o \textit{REINFORCE} teve um bom resultado em relação ao \textit{SARSA} e \textit{Q-Learning}. Isto se deve ao fato de o REINFORCE fazer uma busca eficiente diretamente no espaço de políticas e por trabalhar com aproximação de função, lidar melhor com representações de estado contínuas, enquanto os algoritmos  baseados em métodos tabulares dependem de técnicas de discretização dos estados, podendo perder informações importantes das variáveis de estado para a obtenção de uma política ótima.

Entretanto para se conseguir um bom resultado o \textit{REINFORCE} demandou de capacidade computacional que o que \textit{SARSA} e \textit{Q-Learning}, demandando em até 30 vezes mais tempo para encontrar uma boa política. Um fato que pode explicar esse comportamento é o fato de, enquanto os métodos tabulares atualizam sua estimativa da função $Q(s,a)$ com uma operação matemática mais simples para este problema a cada passo, o \textit{REINFORCE} precisa realizar um ajuste nas redes neurais que estimam $\theta$ e $w$.

Apesar das ressalvas comentadas para o \textit{Fitt-Q}, ele apresenta uma boa performance em relação ao seu tempo de execução, treinando suas redes neurais internas considerando uma experiência inteira ao invés de somente um passo e obtendo melhores valores de passos sucessivos com sucesso que os algoritmos \textit{SARSA} e \textit{Q-Learning}. Entretanto, para uma comparação mais justa entre esses algoritmos, a implementação do \textit{Fitt-Q} deveria ser ajustada para usar um simulador com menos restrições que o dos outros algoritmos.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\columnwidth]{exp-allalgs-episodes}
  \caption{Passos por episódio para os algoritmos}
  \label{fig:exp-allalgs-episodes}
\end{figure}

\section{Conclusão}

Este trabalho teve como objetivo fazer uma breve revisão teórica sobre alguns algoritmos de aprendizado por reforço, bem como realizar alguns experimentos com alguns algoritmos conhecidos.

Os algoritmos baseados em métodos tabulares, \textit{SARSA} e \textit{Q-Learning}, tem bases teóricas importantes para a área de aprendizado por reforço como o aprendizado baseado em métodos de Monte Carlo, onde a experiência de interação do agente é levada em consideração para a obtenção de uma política ótima e é possível se abstrair de certas características de um MDP, como o conhecimento da função de transição e de todos os estados de um problema. Nos experimentos executados esses algoritmos apresentaram limitações para aprender o domínio do simulador devido a sua modelagem considerar variáveis continuas, que mesmo delimitadas por um domínio, apresentam uma combinação muito grande de estados.

Já os algoritmos baseados em aproximação de função, \textit{REINFORCE com baseline} e \textit{Fitt-Q}, apresentaram bons resultados nos experimentos fazendo que o agente conseguisse executar mais passos consectivos com sucesso, porém com uma maior complexidade de implementação (por conta dos cálculos de gradiente descentes e do uso de redes neurais).

Como trabalhos futuros poderiam ser melhorados dois aspectos deste trabalho: pode-se melhorar os experimentos do \textit{SARSA} e \textit{Q-Learning} para utilizar outros métodos de discretização das variáveis de estado ou ainda implementar uma das versões destes algoritmos com aproximação de função e no \textit{Fitt-Q} pode-se melhorar sua implementação para desconsiderar as restrições utilizadas no seu simulador.

\bibliography{references.bib}
\bibliographystyle{aaai}

\end{document}
